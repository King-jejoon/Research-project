{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c2b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "class TanhActivation:\n",
    "    def forward(self, x):\n",
    "        self.output = np.tanh(x)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (1 - self.output ** 2)\n",
    "\n",
    "class LinearActivation:\n",
    "    def forward(self, x):\n",
    "        self.output = x\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "# Dense Layer\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        self.grad_weights = np.dot(self.input.T, grad_output)\n",
    "        self.grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        return grad_input\n",
    "\n",
    "# Neural Network Model\n",
    "class NeuralDataFusionModel:\n",
    "    def __init__(self, input_dim):\n",
    "        self.layer1 = DenseLayer(input_dim, 5)\n",
    "        self.activation1 = TanhActivation()\n",
    "        self.layer2 = DenseLayer(5, 3)\n",
    "        self.activation2 = TanhActivation()\n",
    "        self.layer3 = DenseLayer(3, 1)\n",
    "        self.activation3 = LinearActivation()\n",
    "        \n",
    "        self.parameters = [\n",
    "            self.layer1.weights, self.layer1.bias,\n",
    "            self.layer2.weights, self.layer2.bias,\n",
    "            self.layer3.weights, self.layer3.bias\n",
    "        ]\n",
    "        self.gradients = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1.forward(x)\n",
    "        out = self.activation1.forward(out)\n",
    "        out = self.layer2.forward(out)\n",
    "        out = self.activation2.forward(out)\n",
    "        out = self.layer3.forward(out)\n",
    "        out = self.activation3.forward(out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        grad = self.activation3.backward(grad_output)\n",
    "        grad = self.layer3.backward(grad)\n",
    "        grad = self.activation2.backward(grad)\n",
    "        grad = self.layer2.backward(grad)\n",
    "        grad = self.activation1.backward(grad)\n",
    "        grad = self.layer1.backward(grad)\n",
    "        \n",
    "        self.gradients = [\n",
    "            self.layer1.grad_weights, self.layer1.grad_bias,\n",
    "            self.layer2.grad_weights, self.layer2.grad_bias,\n",
    "            self.layer3.grad_weights, self.layer3.grad_bias\n",
    "        ]\n",
    "        return grad\n",
    "\n",
    "# Delta function\n",
    "def delta_function(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Loss and Gradient Computation\n",
    "def compute_loss_and_gradients(model, X_norm, K, lambda1=0.001, lambda2=0.001):\n",
    "    num_features, total_samples = X_norm.shape\n",
    "    N = total_samples // K\n",
    "    \n",
    "    loss_term1 = 0.0\n",
    "    loss_term2 = 0.0\n",
    "    loss_term3 = 0.0\n",
    "    \n",
    "    all_HI = []\n",
    "    all_inputs = []\n",
    "    \n",
    "    # Forward pass for all engines\n",
    "    for n in range(N):\n",
    "        start_idx = n * K\n",
    "        end_idx = (n + 1) * K\n",
    "        engine_data = X_norm[:, start_idx:end_idx].T\n",
    "        all_inputs.append(engine_data)\n",
    "        \n",
    "        HI_n = model.forward(engine_data)\n",
    "        all_HI.append(HI_n)\n",
    "        \n",
    "        # First term\n",
    "        h_n_Tn = HI_n[-1, 0]\n",
    "        loss_term1 += (h_n_Tn - 1) ** 2\n",
    "        \n",
    "        # Second term: monotonicity\n",
    "        T_n = K\n",
    "        for t in range(1, T_n):\n",
    "            d_nt = HI_n[t, 0] - HI_n[t-1, 0]\n",
    "            exp_d = np.exp(d_nt)\n",
    "            loss_term2 += (1.0 / (T_n - 1)) * max(exp_d - 1, 0)\n",
    "        \n",
    "        # Third term: smoothness\n",
    "        for t in range(2, T_n):\n",
    "            d_nt = HI_n[t, 0] - HI_n[t-1, 0]\n",
    "            d_nt_prev = HI_n[t-1, 0] - HI_n[t-2, 0]\n",
    "            exp_diff = np.exp(d_nt - d_nt_prev)\n",
    "            loss_term3 += (1.0 / (T_n - 2)) * max(exp_diff - 1, 0)\n",
    "    \n",
    "    total_loss = loss_term1 + lambda1 * loss_term2 + lambda2 * loss_term3\n",
    "    \n",
    "    # Compute gradients\n",
    "    total_grad_output = []\n",
    "    \n",
    "    for n in range(N):\n",
    "        HI_n = all_HI[n]\n",
    "        T_n = K\n",
    "        grad_h = np.zeros_like(HI_n)\n",
    "        \n",
    "        # Gradient from first term\n",
    "        h_n_Tn = HI_n[-1, 0]\n",
    "        grad_h[-1, 0] += 2 * (h_n_Tn - 1)\n",
    "        \n",
    "        # Gradient from second term\n",
    "        for t in range(1, T_n):\n",
    "            d_nt = HI_n[t, 0] - HI_n[t-1, 0]\n",
    "            exp_d = np.exp(d_nt)\n",
    "            delta_exp = delta_function(exp_d - 1)\n",
    "            grad_d_nt = (lambda1 / (T_n - 1)) * delta_exp * exp_d\n",
    "            grad_h[t, 0] += grad_d_nt\n",
    "            grad_h[t-1, 0] -= grad_d_nt\n",
    "        \n",
    "        # Gradient from third term\n",
    "        for t in range(2, T_n):\n",
    "            d_nt = HI_n[t, 0] - HI_n[t-1, 0]\n",
    "            d_nt_prev = HI_n[t-1, 0] - HI_n[t-2, 0]\n",
    "            exp_diff = np.exp(d_nt - d_nt_prev)\n",
    "            delta_exp_diff = delta_function(exp_diff - 1)\n",
    "            grad_diff = (lambda2 / (T_n - 2)) * delta_exp_diff * exp_diff\n",
    "            grad_h[t, 0] += grad_diff\n",
    "            grad_h[t-1, 0] -= 2 * grad_diff\n",
    "            grad_h[t-2, 0] += grad_diff\n",
    "        \n",
    "        total_grad_output.append(grad_h)\n",
    "    \n",
    "    # Backpropagation\n",
    "    for n in range(N):\n",
    "        engine_data = all_inputs[n]\n",
    "        grad_output = total_grad_output[n]\n",
    "        _ = model.forward(engine_data)\n",
    "        model.backward(grad_output)\n",
    "        \n",
    "        if n == 0:\n",
    "            accumulated_grads = [g.copy() for g in model.gradients]\n",
    "        else:\n",
    "            for i in range(len(accumulated_grads)):\n",
    "                accumulated_grads[i] += model.gradients[i]\n",
    "    \n",
    "    for i in range(len(accumulated_grads)):\n",
    "        accumulated_grads[i] /= N\n",
    "    \n",
    "    model.gradients = accumulated_grads\n",
    "    return total_loss\n",
    "\n",
    "# Adam Optimizer\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, parameters, alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = [np.zeros_like(p) for p in parameters]\n",
    "        self.v = [np.zeros_like(p) for p in parameters]\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self, parameters, gradients):\n",
    "        self.t += 1\n",
    "        for i in range(len(parameters)):\n",
    "            g = gradients[i]\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * g\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (g ** 2)\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            parameters[i] -= self.alpha * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "# Training Function\n",
    "def train_model(X_norm, K, epochs=1000, lambda1=0.001, lambda2=0.001, \n",
    "                alpha=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, \n",
    "                verbose=True, print_every=100):\n",
    "    num_features = X_norm.shape[0]\n",
    "    model = NeuralDataFusionModel(input_dim=num_features)\n",
    "    optimizer = AdamOptimizer(model.parameters, alpha, beta1, beta2, epsilon)\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = compute_loss_and_gradients(model, X_norm, K, lambda1, lambda2)\n",
    "        loss_history.append(loss)\n",
    "        optimizer.step(model.parameters, model.gradients)\n",
    "        \n",
    "        if verbose and (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.6f}\")\n",
    "    \n",
    "    return model, loss_history\n",
    "\n",
    "# Prediction Function\n",
    "def predict(model, X_norm, K):\n",
    "    num_features, total_samples = X_norm.shape\n",
    "    N = total_samples // K\n",
    "    predictions = []\n",
    "    \n",
    "    for n in range(N):\n",
    "        start_idx = n * K\n",
    "        end_idx = (n + 1) * K\n",
    "        engine_data = X_norm[:, start_idx:end_idx].T\n",
    "        HI_n = model.forward(engine_data)\n",
    "        predictions.append(HI_n.flatten())\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ce86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "model, loss_history = train_model(\n",
    "    X_norm, \n",
    "    K, \n",
    "    epochs=1000,\n",
    "    lambda1=0.001,\n",
    "    lambda2=0.001,\n",
    "    alpha=0.001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=1e-8,\n",
    "    verbose=True,\n",
    "    print_every=100\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
