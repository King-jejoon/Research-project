{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d27a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c2b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.W1 = np.random.randn(input_size, hidden_size1) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size1))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size1, hidden_size2) * 0.01\n",
    "        self.b2 = np.zeros((1, hidden_size2))\n",
    "\n",
    "        self.W3 = np.random.randn(hidden_size2, output_size) * 0.01\n",
    "        self.b3 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = X  # backward에서 사용\n",
    "        self.Z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.A1 = self.tanh(self.Z1)\n",
    "\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = self.tanh(self.Z2)\n",
    "\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        return self.Z3\n",
    "\n",
    "    def tanh(self, Z):\n",
    "        return np.tanh(Z)\n",
    "\n",
    "    def loss_function(self, h, Tn, lambda1, lambda2):\n",
    "\n",
    "        loss = 0.0\n",
    "\n",
    "        # (1) terminal constraint\n",
    "        loss += (h[Tn-1] - 1.0) ** 2\n",
    "\n",
    "        # (2) monotonicity penalty\n",
    "        mono_pen = 0.0\n",
    "        for t in range(1, Tn):\n",
    "            d = h[t-1] - h[t]\n",
    "            if d > 0:\n",
    "                mono_pen += (np.exp(d) - 1)\n",
    "\n",
    "        loss += lambda1 / (Tn - 1) * mono_pen\n",
    "\n",
    "        # (3) convexity penalty\n",
    "        conv_pen = 0.0\n",
    "        for t in range(2, Tn):\n",
    "            d_t   = h[t-1] - h[t]\n",
    "            d_t_1 = h[t-2] - h[t-1]\n",
    "            if (d_t - d_t_1) > 0:\n",
    "                conv_pen += (np.exp(d_t - d_t_1) - 1)\n",
    "\n",
    "        loss += lambda2 / (Tn - 2) * conv_pen\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward_loss(h, Tn, lambda1, lambda2):\n",
    "\n",
    "        grad_h = np.zeros_like(h)\n",
    "\n",
    "        # ---------- (1) terminal constraint ----------\n",
    "        # (h[Tn-1] - 1)^2\n",
    "        grad_h[Tn-1] += 2.0 * (h[Tn-1] - 1.0)\n",
    "\n",
    "        # ---------- (2) monotonicity penalty ----------\n",
    "        # d_t = h[t-1] - h[t]\n",
    "        for t in range(1, Tn):\n",
    "            d = h[t-1] - h[t]\n",
    "            if d > 0:\n",
    "                coef = lambda1 / (Tn - 1)\n",
    "                e = np.exp(d)\n",
    "\n",
    "                grad_h[t-1] += coef * e      # ∂d/∂h[t-1] = +1\n",
    "                grad_h[t]   -= coef * e      # ∂d/∂h[t]   = -1\n",
    "\n",
    "        # ---------- (3) convexity penalty ----------\n",
    "        # d_t - d_{t-1}\n",
    "        for t in range(2, Tn):\n",
    "            d_t   = h[t-1] - h[t]\n",
    "            d_t_1 = h[t-2] - h[t-1]\n",
    "            diff = d_t - d_t_1\n",
    "\n",
    "            if diff > 0:\n",
    "                coef = lambda2 / (Tn - 2)\n",
    "                e = np.exp(diff)\n",
    "\n",
    "                grad_h[t-2] += coef * e        # +1\n",
    "                grad_h[t-1] += -2 * coef * e   # -2\n",
    "                grad_h[t]   += coef * e        # +1\n",
    "\n",
    "        return grad_h\n",
    "\n",
    "\n",
    "    def update(theta, qk, m, v, k, alpha, beta1, beta2, eps):\n",
    "        \"\"\"\n",
    "        theta : parameter vector\n",
    "        qk    : gradient or subgradient\n",
    "        m     : 1st moment vector\n",
    "        v     : 2nd moment vector\n",
    "        k     : iteration index (int, starts from 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # update moments\n",
    "        m = beta1 * m + (1 - beta1) * qk\n",
    "        v = beta2 * v + (1 - beta2) * (qk ** 2)\n",
    "\n",
    "        # bias correction\n",
    "        m_hat = m / (1 - beta1 ** k)\n",
    "        v_hat = v / (1 - beta2 ** k)\n",
    "\n",
    "        # parameter update\n",
    "        theta = theta - alpha * m_hat / (np.sqrt(v_hat) + eps)\n",
    "\n",
    "        return theta, m, v\n",
    "\n",
    " \n",
    "    def train_step(self, X, Y, lambda1, lambda2, lr=0.1):\n",
    "        Y_hat = self.forward(X)\n",
    "        loss = self.loss_function(Y_hat, Y.shape[0], lambda1, lambda2)\n",
    "        self.update(lr)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ce86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(input_size=10, hidden_size=32, output_size=5, seed=0)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss = mlp.train_step(X_train, y_train, lr=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_prac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
